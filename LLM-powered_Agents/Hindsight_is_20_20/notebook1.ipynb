{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'content': None,\n",
       "  'tool_calls': [ChatCompletionMessageToolCall(id='call_DAagYgst7cAZN81jiREMgZXR', function=Function(arguments='{\"query\": \"former president of Korea\"}', name='YDCSearch'), type='function'),\n",
       "   ChatCompletionMessageToolCall(id='call_d65H5JWiKyYX4pGjQmLaALd0', function=Function(arguments='{\"query\": \"MLB American League MVP 2022\"}', name='YDCSearch'), type='function')]},\n",
       " {'role': 'assistant',\n",
       "  'content': None,\n",
       "  'tool_calls': [ChatCompletionMessageToolCall(id='call_IYKp4eH8NlfugQ6TfMhaZTCq', function=Function(arguments='{\"query\": \"former president of South Korea\"}', name='YDCSearch'), type='function'),\n",
       "   ChatCompletionMessageToolCall(id='call_UFla1MxRdEhBfytkDDJQ5wXL', function=Function(arguments='{\"query\": \"MLB American League MVP 2022\"}', name='YDCSearch'), type='function')]},\n",
       " {'role': 'assistant',\n",
       "  'content': None,\n",
       "  'tool_calls': [ChatCompletionMessageToolCall(id='call_fXiuPNqs9Hwj65KN93bB2Mbf', function=Function(arguments='{\"query\": \"former president of Korea\"}', name='YDCSearch'), type='function'),\n",
       "   ChatCompletionMessageToolCall(id='call_m32TNfEkJYePsD16mErAc7eF', function=Function(arguments='{\"query\": \"MLB American League MVP last season\"}', name='YDCSearch'), type='function')]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from reasoning_engines.langchain_llm_wrappers import QuickAzureChatOpenAI, QuickAzureOpenAIClient\n",
    "from tools.get_tools import DefaultSchemasTools\n",
    "from langchain import hub\n",
    "from utils.load_envs import Environ\n",
    "from chains.qa_evaluators import CustomQAEvaluator\n",
    "\n",
    "import os\n",
    "from reasoning_engines.langchain_llm_wrappers import QuickAzureChatOpenAI, QuickAzureOpenAIClient, AsyncQuickAzureOpenAIClient\n",
    "from tools.get_tools import DefaultSchemasTools\n",
    "from langchain import hub\n",
    "from utils.load_envs import Environ\n",
    "from chains.qa_evaluators import CustomQAEvaluator\n",
    "import asyncio\n",
    "\n",
    "#### Parallel Function Calling Agent\n",
    "GPT_VER = '1106' \n",
    "from agents.parallel_func_calling_agent import OpenAIParallelFuntionCallingAgent\n",
    "parallel_calling_agent = OpenAIParallelFuntionCallingAgent(\n",
    "    reasoninig_engine = QuickAzureChatOpenAI(version=GPT_VER),\n",
    "    base_prompt = 'jet-taekyo-lee/parallel-function-calling-agent',\n",
    "    schemas_and_tools = DefaultSchemasTools(azure_gpt_version=GPT_VER),\n",
    "    evaluator = CustomQAEvaluator(llm=QuickAzureChatOpenAI('0613')),\n",
    "    action_word = 'Tool invocations in parallel',\n",
    "    observation_word = 'Tool-invocations results in parallel',\n",
    "    use_chat_completion_api = True,\n",
    "    azure_apenai_client = AsyncQuickAzureOpenAIClient(GPT_VER),\n",
    "    horizon = 5\n",
    ")\n",
    "\n",
    "query=\"Who is older? The former president of Korea vs. the MLB American League MVP of the last season. What is the result if you sum up the ages of the two?\"\n",
    "query_hello=\"Hello\"\n",
    "queries = [query, query_hello]\n",
    "\n",
    "responses = await asyncio.gather( *[parallel_calling_agent._invoke_agent_action_async(q) for q in queries] ) \n",
    "responses\n",
    "\n",
    "\n",
    "\n",
    "# parallel_calling_agent.clear_logs()\n",
    "# parallel_calling_agent.run_agent_trials(num_trials=2, \n",
    "# query=\"Who is older? The former president of Korea vs. the MLB American League MVP of the last season. What is the result if you sum up the ages of the two?\",\n",
    "# reference=\"The former former president of Korea is older. The sum of their age is 99.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QuickAzureOpenAIClient('1106')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "query=\"How are the current weathers in Seoul and Tokyo?\"\n",
    "\n",
    "await client.chat_completions_create_async(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoning_engines.langchain_llm_wrappers import AsyncQuickAzureOpenAIClient, QuickAzureOpenAIClient\n",
    "from utils.load_envs import Environ\n",
    "env = Environ()\n",
    "await env.openai_async()\n",
    "message = [{\n",
    "    'role':'assistant',\n",
    "    'content':'You are a helpful assistant.'\n",
    "},\n",
    "{\n",
    "    'role':'user',\n",
    "    'content':'Who are the current weather in Seoul and New York?'\n",
    "}]\n",
    "\n",
    "from tools.get_tools import DefaultSchemasTools\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_tool\n",
    "\n",
    "schema_and_tool = DefaultSchemasTools('1106')\n",
    "schemas = schema_and_tool.schemas()\n",
    "\n",
    "openai_functions = [convert_pydantic_to_openai_tool(x) for x in schemas]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reasoning_engines.langchain_llm_wrappers.AsyncQuickAzureOpenAIClient"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "async_client = AsyncQuickAzureOpenAIClient('1106')\n",
    "type(async_client)\n",
    "\n",
    "# result = await asyncio.gather(*[async_client.chat_completions_create(messages=x, tools=openai_functions) for x in [message, message, message]])\n",
    "# result\n",
    "# await async_client.chat_completions_create(messages=message, tools=openai_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QuickAzureOpenAIClient('1106')\n",
    "result = [client.chat_completions_create(messages=x, tools=openai_functions) for x in [message, message, message]]\n",
    "# client.chat_completions_create(messages=message, tools=openai_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_l5k9K4UK3jhlYzOHC8nlQbIB', function=Function(arguments='{\"IANA_timezone\": \"Asia/Seoul\"}', name='OpenWeatherMap'), type='function'), ChatCompletionMessageToolCall(id='call_s046XITbbygMQ8NMnuy0WBK7', function=Function(arguments='{\"IANA_timezone\": \"America/New_York\"}', name='OpenWeatherMap'), type='function')]),\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_kLcrHl2spYKC5hknAxDQeNJ6', function=Function(arguments='{\"IANA_timezone\": \"Asia/Seoul\"}', name='OpenWeatherMap'), type='function'), ChatCompletionMessageToolCall(id='call_haONAvZ5vjbHZm1NezfjqBGg', function=Function(arguments='{\"IANA_timezone\": \"America/New_York\"}', name='OpenWeatherMap'), type='function')]),\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_l5k9K4UK3jhlYzOHC8nlQbIB', function=Function(arguments='{\"IANA_timezone\": \"Asia/Seoul\", \"Days_from_now\": 0}', name='OpenWeatherMap'), type='function'), ChatCompletionMessageToolCall(id='call_s046XITbbygMQ8NMnuy0WBK7', function=Function(arguments='{\"IANA_timezone\": \"America/New_York\", \"Days_from_now\": 0}', name='OpenWeatherMap'), type='function')])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "\n",
    "async def get_value(a):    \n",
    "    return sum([a['a'], 100])\n",
    "\n",
    "async def get_value_acync(a):\n",
    "    result = sum([a, 100])\n",
    "    return result\n",
    "\n",
    "a = 100\n",
    "\n",
    "await get_value_acync(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "a = '{\"a\":3}'\n",
    "json.loads(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('llm_agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3fb45ff54312d810d3b33eec9af47862c42661f076ce6b0d7045a13e18ceae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
