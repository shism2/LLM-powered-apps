{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Query: What is the summation of the temperatures of Seoul, Tokyo, and New york?\n",
      "[INFO] Trial 1\n",
      "[INFO] Tool invocations in parallel (step 1): [Tool 1-> OpenWeatherMap(IANA_timezone=Asia/Seoul), Tool 2-> OpenWeatherMap(IANA_timezone=Asia/Tokyo), Tool 3-> OpenWeatherMap(IANA_timezone=America/New_York)]\n",
      "[INFO] Results in parallel (step 1): [Tool 1-> timezone: Seoul, current time(year-month-day): 2023-11-29, temperature: 1.2300000000000182, humidity: 53, wind_speed: 6.22, weather_summary: Expect a day of partly cloudy with snow., Tool 2-> timezone: Tokyo, current time(year-month-day): 2023-11-29, temperature: 13.520000000000039, humidity: 35, wind_speed: 5.51, weather_summary: You can expect clear sky in the morning, with partly cloudy in the afternoon., Tool 3-> timezone: New York County, current time(year-month-day): 2023-11-29, temperature: 1.0500000000000114, humidity: 42, wind_speed: 7.23, weather_summary: The day will start with clear sky through the late morning hours, transitioning to partly cloudy.]\n",
      "[INFO] Tool invocations in parallel (step 2): [Tool 1-> PythonREPL(command=print(1.2300000000000182 + 13.520000000000039 + 1.0500000000000114))]\n",
      "Python REPL can execute arbitrary code. Use with caution.\n",
      "[INFO] Results in parallel (step 2): [Tool 1-> 15.800000000000068\n",
      "]\n",
      "[INFO] Tool invocations in parallel (step 3): Now I can answer directly without resorting to any tool.\n",
      "[INFO] Answer: The summation of the temperatures of Seoul, Tokyo, and New York is approximately 15.8Â°C.\n",
      "[INFO] Jugdement: You failed to provide an answer because you exceeded the permitted number of reasoning steps. You must give an answer within 3 steps.\n"
     ]
    }
   ],
   "source": [
    "from reasoning_engines.langchain_llm_wrappers import QuickAzureChatOpenAI\n",
    "from reasoning_engines.langchain_llm_wrappers import QuickOpenAIClient\n",
    "from reasoning_engines.langchain_llm_wrappers import QuickAzureOpenAIClient\n",
    "openai_client = QuickOpenAIClient()\n",
    "azure_openai_client = QuickAzureOpenAIClient('1106')\n",
    "llm = QuickAzureChatOpenAI('1106')\n",
    "# openai_response = openai_client.chat_completions_create(query='How is the weather in Seoul, Bangkok and LA?', tools=openai_functions)\n",
    "# azure_openai_client.chat_completions_create(query='hello')\n",
    "from tools.get_tools import DefaultSchemasTools\n",
    "default_schemas_and_tools = DefaultSchemasTools(azure_gpt_version='1106')\n",
    "schemas_and_tools = default_schemas_and_tools.schemas_and_tools()\n",
    "schemas = default_schemas_and_tools.schemas()\n",
    "tools = default_schemas_and_tools.tools()\n",
    "tool_dictionary = default_schemas_and_tools.tool_dictionary()\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_tool\n",
    "openai_functions = [convert_pydantic_to_openai_tool(x) for x in schemas]\n",
    "\n",
    "\n",
    "from utils.wrappers import retry\n",
    "from openai import RateLimitError\n",
    "@retry(allowed_exceptions=(RateLimitError,))\n",
    "def get_function_response(function_name, function_args):\n",
    "    return tool_dictionary[function_name].run(**function_args)\n",
    "\n",
    "\n",
    "from agents.parallel_func_calling_agent import OpenAIParallelFuntionCallingAgent\n",
    "from chains.qa_evaluators import CustomQAEvaluator\n",
    "\n",
    "parallel_calling_agent = OpenAIParallelFuntionCallingAgent(\n",
    "    reasoninig_engine = QuickAzureChatOpenAI('1106'),\n",
    "    base_prompt = 'jet-taekyo-lee/parallel-function-calling-agent',\n",
    "    schemas_and_tools = DefaultSchemasTools(azure_gpt_version='1106'),\n",
    "    evaluator = CustomQAEvaluator(llm=QuickAzureChatOpenAI('1106')),\n",
    "    action_word = 'Tool invocations in parallel',\n",
    "    use_chat_completion_api = True,\n",
    "    azure_apenai_client = QuickAzureOpenAIClient('1106'),\n",
    "    horizon = 3\n",
    ")\n",
    "\n",
    "query=\"How old are the current Korean president and Joe Biden?\"\n",
    "# query=\"How old are the current Korean president?\"\n",
    "query=\"What is the summation of the temperatures of Seoul, Tokyo, and New york?\"\n",
    "reference=\"Joe Biden is 81 years old and the current Korean president, Yoon Suk Yeol, is 62 years old.\"\n",
    "parallel_calling_agent.clear_logs()\n",
    "parallel_calling_agent.run_agent_trials(num_trials=1, \n",
    "query=query)\n",
    "# reference=reference)\n",
    "\n",
    "# import time\n",
    "# for _ in range(5):\n",
    "#     for s in range(60, 0, -1):\n",
    "#         print(s, end=' ')\n",
    "#         time.sleep(1)   \n",
    "#     parallel_calling_agent.clear_logs()\n",
    "\n",
    "#     parallel_calling_agent.agent_step(query)\n",
    "    # parallel_calling_agent.run_agent_trials(num_trials=3, \n",
    "    # query=query,\n",
    "    # reference=reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('llm_agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3fb45ff54312d810d3b33eec9af47862c42661f076ce6b0d7045a13e18ceae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
