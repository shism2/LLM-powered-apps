{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from reasoning_engines.langchain_llm_wrappers import QuickAzureChatOpenAI, QuickAzureOpenAIClient\n",
    "from tools.get_tools import DefaultSchemasTools\n",
    "from langchain import hub\n",
    "from utils.load_envs import Environ\n",
    "from chains.qa_evaluators import CustomQAEvaluator\n",
    "\n",
    "import os\n",
    "from reasoning_engines.langchain_llm_wrappers import QuickAzureChatOpenAI, QuickAzureOpenAIClient, AsyncQuickAzureOpenAIClient\n",
    "from tools.get_tools import DefaultSchemasTools\n",
    "from langchain import hub\n",
    "from utils.load_envs import Environ\n",
    "from chains.qa_evaluators import CustomQAEvaluator\n",
    "import asyncio\n",
    "\n",
    "#### Parallel Function Calling Agent\n",
    "GPT_VER = '1106' \n",
    "from agents.parallel_func_calling_agent import OpenAIParallelFuntionCallingAgent\n",
    "parallel_calling_agent = OpenAIParallelFuntionCallingAgent(\n",
    "    reasoninig_engine = QuickAzureChatOpenAI(version=GPT_VER),\n",
    "    base_prompt = 'jet-taekyo-lee/parallel-function-calling-agent',\n",
    "    schemas_and_tools = DefaultSchemasTools(azure_gpt_version=GPT_VER),\n",
    "    evaluator = CustomQAEvaluator(llm=QuickAzureChatOpenAI('0613')),\n",
    "    action_word = 'Tool invocations in parallel',\n",
    "    observation_word = 'Tool-invocations results in parallel',\n",
    "    use_chat_completion_api = True,\n",
    "    azure_apenai_client = AsyncQuickAzureOpenAIClient(GPT_VER),\n",
    "    horizon = 5\n",
    ")\n",
    "\n",
    "query=\"Who is older? The former president of Korea vs. the MLB American League MVP of the last season. What is the result if you sum up the ages of the two?\"\n",
    "query=\"Who is The former former president of Korea?\"\n",
    "query_hello=\"Hello\"\n",
    "# queries = [query, query_hello]\n",
    "queries = [query]\n",
    "\n",
    "# responses = await asyncio.gather( *[parallel_calling_agent._invoke_agent_action_async(q) for q in queries] ) \n",
    "# responses\n",
    "\n",
    "# responses  = await asyncio.gather( *[parallel_calling_agent.agent_step_async(q) for q in queries] ) \n",
    "# responses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# parallel_calling_agent.clear_logs()\n",
    "# parallel_calling_agent.run_agent_trials(num_trials=2, \n",
    "# query=\"Who is older? The former president of Korea vs. the MLB American League MVP of the last season. What is the result if you sum up the ages of the two?\",\n",
    "# reference=\"The former former president of Korea is older. The sum of their age is 99.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful assistant. Respond to the human as helpfully and accurately as possible in the same language as the human. But your intermediate processes should be done in English for more decent results. Answer in a consise manner with only a few words or a sentence if possible.'},\n",
       " {'role': 'user', 'content': ''},\n",
       " {'role': 'system',\n",
       "  'content': 'You are a helpful assistant. Respond to the human as helpfully and accurately as possible in the same language as the human. But your intermediate processes should be done in English for more decent results. Answer in a consise manner with only a few words or a sentence if possible.'},\n",
       " {'role': 'user', 'content': ''}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_calling_agent.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "class Animal:\n",
    "    def __init__(self):\n",
    "        self.queue = []\n",
    "\n",
    "    async def func(self, a):\n",
    "        self.queue.append(a)\n",
    "        await asyncio.sleep(1)\n",
    "        print(self.queue.pop())\n",
    "\n",
    "    async def main(self, num):\n",
    "        resilt = await asyncio.gather(*[self.func(i) for i in range(num)])\n",
    "\n",
    "animal = Animal() \n",
    "\n",
    "await animal.main(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('llm_agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3fb45ff54312d810d3b33eec9af47862c42661f076ce6b0d7045a13e18ceae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
