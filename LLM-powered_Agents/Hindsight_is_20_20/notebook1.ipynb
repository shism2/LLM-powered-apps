{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Query: How old are the current Korean president and Joe Biden?\n",
      "[INFO] Trial 1\n",
      "[INFO] (step 1-1) Tool invocations in parallel: [Tool 1-> YDCSearch(query=current Korean president age), Tool 2-> YDCSearch(query=Joe Biden age)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RateLimitError for _get_function_observation. -----> Will automatically retry 56 seconds later.\n",
      "56 55 54 53 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] (step 1-2) Results in parallel: [Tool 1-> Yoon Suk Yeol, the current president of South Korea, was born on December 18, 1960. As of the knowledge cutoff date in 2023, he would be 62 years old., Tool 2-> Joe Biden is 81 years old. He was born on November 20, 1942.]\n",
      "[INFO] (step 2-1) Tool invocations in parallel: Now I can answer directly without resorting to any tool.\n",
      "[INFO] Answer: Korean President Yoon Suk Yeol is 62 years old, and Joe Biden is 81 years old.\n",
      "[INFO] Jugdement: Your answer is correct.\n"
     ]
    }
   ],
   "source": [
    "from reasoning_engines.langchain_llm_wrappers import QuickAzureChatOpenAI\n",
    "from reasoning_engines.langchain_llm_wrappers import QuickOpenAIClient\n",
    "from reasoning_engines.langchain_llm_wrappers import QuickAzureOpenAIClient\n",
    "openai_client = QuickOpenAIClient()\n",
    "azure_openai_client = QuickAzureOpenAIClient('1106')\n",
    "llm = QuickAzureChatOpenAI('1106')\n",
    "# openai_response = openai_client.chat_completions_create(query='How is the weather in Seoul, Bangkok and LA?', tools=openai_functions)\n",
    "# azure_openai_client.chat_completions_create(query='hello')\n",
    "from tools.get_tools import DefaultSchemasTools\n",
    "default_schemas_and_tools = DefaultSchemasTools(azure_gpt_version='1106')\n",
    "schemas_and_tools = default_schemas_and_tools.schemas_and_tools()\n",
    "schemas = default_schemas_and_tools.schemas()\n",
    "tools = default_schemas_and_tools.tools()\n",
    "tool_dictionary = default_schemas_and_tools.tool_dictionary()\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_tool\n",
    "openai_functions = [convert_pydantic_to_openai_tool(x) for x in schemas]\n",
    "\n",
    "\n",
    "from utils.wrappers import retry\n",
    "from openai import RateLimitError\n",
    "@retry(allowed_exceptions=(RateLimitError,))\n",
    "def get_function_response(function_name, function_args):\n",
    "    return tool_dictionary[function_name].run(**function_args)\n",
    "\n",
    "\n",
    "from agents.parallel_func_calling_agent import OpenAIParallelFuntionCallingAgent\n",
    "from chains.qa_evaluators import CustomQAEvaluator\n",
    "\n",
    "parallel_calling_agent = OpenAIParallelFuntionCallingAgent(\n",
    "    reasoninig_engine = QuickAzureChatOpenAI('1106'),\n",
    "    base_prompt = 'jet-taekyo-lee/parallel-function-calling-agent',\n",
    "    schemas_and_tools = DefaultSchemasTools(azure_gpt_version='1106'),\n",
    "    evaluator = CustomQAEvaluator(llm=QuickAzureChatOpenAI('1106')),\n",
    "    action_word = 'Tool invocations in parallel',\n",
    "    use_chat_completion_api = True,\n",
    "    azure_apenai_client = QuickAzureOpenAIClient('1106'),\n",
    "    horizon = 3\n",
    ")\n",
    "\n",
    "query=\"How old are the current Korean president and Joe Biden?\"\n",
    "# query=\"How old are the current Korean president?\"\n",
    "# query=\"What is the summation of the temperatures of Seoul, Tokyo, and New york?\"\n",
    "reference=\"Joe Biden is 81 years old and the current Korean president, Yoon Suk Yeol, is 62 years old.\"\n",
    "parallel_calling_agent.clear_logs()\n",
    "\n",
    "\n",
    "parallel_calling_agent.run_agent_trials(num_trials=1, \n",
    "query=query,\n",
    "reference=reference)\n",
    "\n",
    "# import time\n",
    "# for _ in range(5):\n",
    "#     for s in range(60, 0, -1):\n",
    "#         print(s, end=' ')\n",
    "#         time.sleep(1)   \n",
    "#     parallel_calling_agent.clear_logs()\n",
    "\n",
    "#     parallel_calling_agent.agent_step(query)\n",
    "    # parallel_calling_agent.run_agent_trials(num_trials=3, \n",
    "    # query=query,\n",
    "    # reference=reference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('llm_agents')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3fb45ff54312d810d3b33eec9af47862c42661f076ce6b0d7045a13e18ceae3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
